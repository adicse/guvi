# -*- coding: utf-8 -*-
"""guvi_MP-Neuron_and_perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12EGvjjbbWTBTzk4w2Pa7Ood8UbCcAsYw
"""



"""# New Section"""

# Import PyDrive and associated libraries.
# This only needs to be done once in a notebook.
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
# This only needs to be done once in a notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Create & upload a text file.
uploaded = drive.CreateFile({'title': 'Sample file.txt'})
uploaded.SetContentString('Sample upload file content')
uploaded.Upload()
print('Uploaded file with ID {}'.format(uploaded.get('id')))

import sklearn.datasets
import numpy as np

ds_breast_cancer = sklearn.datasets.load_breast_cancer()

X = ds_breast_cancer.data
Y = ds_breast_cancer.target
print('X shape:', X.shape, 'Y shape:', Y.shape)

import pandas as pd
df_data = pd.DataFrame(ds_breast_cancer.data, columns=ds_breast_cancer.feature_names)
df_data['class'] = Y
df_data.head()



df_data.describe()

print(df_data['class'].value_counts())
print(ds_breast_cancer.target_names)

# split data into training data and test data
from sklearn.model_selection import train_test_split

X = df_data.drop('class', axis = 1)
Y = df_data['class']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y)
print(X.shape, X_train.shape, X_test.shape )

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1 ) # train data : test data  = ( 9 : 1 )
print(X.shape, X_train.shape, X_test.shape )

print(Y.shape, Y_train.shape, Y_test.shape )

print(Y.mean(), Y_train.mean(), Y_test.mean() )
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, stratify = Y ) # no arbitraty splitting, it tries to maintain the mean
print(Y.mean(), Y_train.mean(), Y_test.mean() )

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, stratify = Y, random_state = 1 ) #random_state is some constant number
print(X.mean(), X_train.mean(), X_test.mean() )

# binarization since mp MP neuron and perceptron accept binary data
# lets first plot data

import matplotlib.pyplot as plt
#plot each of the features
#plt.plot( X_train, '*')

#plt each value in a column
plt.plot(X_train.T,'*')
plt.xticks(rotation='vertical')
plt.show()

#plt each value in a column
plt.plot(X_test.T,'*')
plt.xticks(rotation='vertical')
plt.show()

#binarise whole pandas train data frame
X_binarised_train = X_train.apply(pd.cut,bins=2, labels=[0,1])
X_binarised_test = X_test.apply(pd.cut,bins=2, labels=[0,1])

#check the type of binarised dataframe
print( 'X_binarised_train type:', type( X_binarised_train) )
print( 'X_binarised_train type:', type(X_binarised_test) )

#convert dataframe to numpy array
X_binarised_train = X_binarised_train.values
X_binarised_test = X_binarised_test.values
print('X_binarised_train values type: ', type(X_binarised_train) )
print('X_binarised_test values type: ', type(X_binarised_test) )

# lets use MP neuron 

b = 3 # arbitrary value
i = 100


if np.sum(X_binarised_train[i:]) >= b: # condition to predict if the cancer is malignant or Benign
  print('MP Neuron intereference is Malignant')
else:
  print('MP Neuron intereference is Benign')

# cross check the prediction with actual data
if Y_train[i] == 1:
  print('Ground Truth is Malignant')
else:
  print('Ground Truth is Benign')

# now lets predict for more data
from random import randint

Y_pred_train = []
accurate_rows = 0
for x,y in zip(X_binarised_train, Y_train):
  Y_pred = np.sum(x) >= b
  Y_pred_train.append( Y_pred )
  accurate_rows += ( y == Y_pred)

print('accurate rows found:', accurate_rows, 'accuracy: ', accurate_rows/X_binarised_train.shape[0]*100)

# we have quite low accuracy, so we need to find the value of b which gives better accuracy
for b in range(X_binarised_train.shape[0] + 1):
  Y_pred_train = []
  accurate_rows = 0
  for x,y in zip(X_binarised_train, Y_train):
    Y_pred = np.sum(x) >= b
    Y_pred_train.append( Y_pred )
    accurate_rows += ( y == Y_pred)
  print('b:',b,'accurate rows found:', accurate_rows, 'accuracy: ', accurate_rows/X_binarised_train.shape[0]*100)


# the model can not do good, the accuracy becomes constantfrom b = 18
# for explanation have a look @ https://www.guvi.in/courses-video?course=dl_primitive_neurons# MP Neurons & Perceptron using Python: Inference And Search
# video at 6:57
# earlier we had an assumption which came out to be wrong

#binarise whole pandas train data frame
X_binarised_train = X_train.apply(pd.cut,bins=2, labels=[1,0])
X_binarised_test = X_test.apply(pd.cut,bins=2, labels=[1,0])

#check the type of binarised dataframe
print( 'X_binarised_train type:', type( X_binarised_train) )
print( 'X_binarised_train type:', type(X_binarised_test) )

#convert dataframe to numpy array
X_binarised_train = X_binarised_train.values
X_binarised_test = X_binarised_test.values
print('X_binarised_train values type: ', type(X_binarised_train) )
print('X_binarised_test values type: ', type(X_binarised_test) )

# now the correction has been made
for b in range(X_binarised_train.shape[0] + 1):
  Y_pred_train = []
  accurate_rows = 0
  for x,y in zip(X_binarised_train, Y_train):
    Y_pred = np.sum(x) >= b
    Y_pred_train.append( Y_pred )
    accurate_rows += ( y == Y_pred)
  print('b:',b,'accurate rows found:', accurate_rows, 'accuracy: ', accurate_rows/X_binarised_train.shape[0]*100)

# best accuracy is 84.95 @ b = 28
# b: 28 accurate rows found: 435 accuracy:  84.9609375
# however here also the accuracy decreases and becomes constant after sometime

# now use the test data to verify and use sklearn inbuild method to calculate accuracy
from sklearn.metrics import accuracy_score
b = 28
Y_pred_test = []
for x in X_binarised_test:
  Y_pred = np.sum(x) >= b
  Y_pred_test.append( Y_pred )
accuracy = accuracy_score(Y_pred_test, Y_test)
print('accuracy on test data is:', accuracy)

"""here we see that the accuracy for test data is lower than the trainig data
there may be a diff value for b which gives better accuracy 
but we stick with b computed based on the train data

**MPNeuron Class**
"""

class MPNeuron:
  def __init__(self):
    self.b = None

  def model(self, x):
    #function which will make the prediction
    return (sum(x) >= self.b)

  # the prediciton algo which predicts the output
  def predict(self, X): # X is whole set of input rows
    Y = []
    for x in X: #x is one row from the input
      Y.append(self.model(x))
    return np.array(Y) # why return np array

  # the learning algorithm, which finds the value of b
  def fit(self, X, Y):
    #the fit operation is run on binarised train dataset
    accuracy = {}
    for b in range(X.shape[0] + 1):
      self.b = b # this is required to be set since need to find the accuracy for every value for  b
      Y_pred = self.predict(X)  # get the values over all the data
      accuracy[b] = accuracy_score( Y_pred, Y)
    # now to find the b for which the accuracy is highest
    b = max(accuracy, key = accuracy.get)
    self.b = b
    print('Highest accuracy for b =', b, ' accuracy is:', accuracy[b])

mp_neuron = MPNeuron()
mp_neuron.fit(X_binarised_train, Y_train)

Y_test_predict = mp_neuron.predict(X_binarised_test)
accuracy = accuracy_score(Y_test_predict, Y_test )
print(mp_neuron.b, accuracy)

"""# **Perceptron**

Introducing weights to mp neuron

$y = 1, \mbox{if} \sum_i w_i x_i >= b$ 

$y = 0, \mbox{if} \sum_i w_i x_i < b$
"""

X_train_nparray = X_train.values
X_test_nparray = X_test.values
type(X_train_nparray)

class Perceptron:
  def __init__(self):
    self.w = None # vector or array
    self.b = None

  def model(self, x):
    # logic to get the output for a row
  
    return 1 if (np.dot(self.w, x) >= self.b ) else 0

  def predict(self, X):
    # guess/predict the output for the whole array
    Y = []
    for x in X:
      Y.append( self.model(x) )    
    return np.array(Y)

  def fit(self, X, Y):
    #learning algo, the train dataset
    self.w = np.ones( X.shape[1] )
    self.b = 0

    for x, y in zip(X,Y):
      y_pred = self.model(x)
      if y_pred == 0 and y == 1:
        self.w = self.w + x
        self.b = self.b + 1
      elif y_pred == 1 and y == 0:
        self.w = self.w - x
        self.b = self.b - 1
      
  def fit_epoch(self, X, Y, epochs = 1):
    #learning algo, the train dataset
    self.w = np.ones( X.shape[1] )
    self.b = 0

    accuracy = {}

    for i in range( epochs ):
      for x, y in zip(X,Y):
        y_pred = self.model(x)
        if y_pred == 0 and y == 1:
          self.w = self.w + x
          self.b = self.b + 1
        elif y_pred == 1 and y == 0:
          self.w = self.w - x
          self.b = self.b - 1
      accuracy[ i ] = accuracy_score(self.predict(X), Y)
    
    list_accuracy = list(accuracy.values() ) 
    print('max accuracy: ', max( list_accuracy) )
    plt.plot( list_accuracy )
    plt.show()

  def fit_epoch_chkpt(self, X, Y, epochs = 1):
    #learning algo, the train dataset
    self.w = np.ones( X.shape[1] )
    self.b = 0

    accuracy = {}
    chkpt_w = None
    chkpt_b = None
    max_accuracy = 0
    for i in range( epochs ):
      for x, y in zip(X,Y):
        y_pred = self.model(x)
        if y_pred == 0 and y == 1:
          self.w = self.w + x
          self.b = self.b + 1
        elif y_pred == 1 and y == 0:
          self.w = self.w - x
          self.b = self.b - 1
      accuracy[ i ] = accuracy_score(self.predict(X), Y)
      if accuracy[ i ] > max_accuracy:
        max_accuracy = accuracy[i]
        chkpt_w = self.w
        ckhpt_b = self.b
    if chkpt_w is not None:
      self.w = chkpt_w
    if chkpt_b is not None:
      self.b = chkpt_b    
    list_accuracy = list(accuracy.values() ) 
    print('max accuracy: ', max_accuracy )
    plt.plot( list_accuracy )
    plt.show()

  def fit_lr(self, X, Y, epochs = 1, lr = 1):
    #learning algo, the train dataset
    self.w = np.ones( X.shape[1] )
    self.b = 0

    accuracy = {}
    chkpt_w = None
    chkpt_b = None
    max_accuracy = 0
    for i in range( epochs ):
      for x, y in zip(X,Y):
        y_pred = self.model(x)
        if y_pred == 0 and y == 1:
          self.w = self.w + x * lr
          self.b = self.b + 1 * lr
        elif y_pred == 1 and y == 0:
          self.w = self.w - x * lr
          self.b = self.b - 1 * lr
      accuracy[ i ] = accuracy_score(self.predict(X), Y)
      if accuracy[ i ] > max_accuracy:
        max_accuracy = accuracy[i]
        chkpt_w = self.w
        ckhpt_b = self.b
    if chkpt_w is not None:
      self.w = chkpt_w
    if chkpt_b is not None:
      self.b = chkpt_b    
    list_accuracy = list(accuracy.values() ) 
    print('max accuracy: ', max_accuracy )
    plt.plot( list_accuracy )
    plt.ylim([0,1])
    plt.show()

  def fit_lr_animate(self, X, Y, epochs = 1, lr = 1):
    #learning algo, the train dataset
    self.w = np.ones( X.shape[1] )
    self.b = 0

    accuracy = {}
    chkpt_w = None
    chkpt_b = None
    max_accuracy = 0
    wt_matrix = []

    for i in range( epochs ):
      for x, y in zip(X,Y):
        y_pred = self.model(x)
        if y_pred == 0 and y == 1:
          self.w = self.w + x * lr
          self.b = self.b + 1 * lr
        elif y_pred == 1 and y == 0:
          self.w = self.w - x * lr
          self.b = self.b - 1 * lr

      wt_matrix.append(self.w)
      accuracy[ i ] = accuracy_score(self.predict(X), Y)
      if accuracy[ i ] > max_accuracy:
        max_accuracy = accuracy[i]
        chkpt_w = self.w
        ckhpt_b = self.b
    if chkpt_w is not None:
      self.w = chkpt_w
    if chkpt_b is not None:
      self.b = chkpt_b    
    list_accuracy = list(accuracy.values() ) 
    print('max accuracy: ', max_accuracy )
    plt.plot( list_accuracy )
    plt.ylim([0,1])
    plt.show()

    return np.array( wt_matrix )
perceptron = Perceptron()
# planning to run on train data, NOT ON BINARISED DATA
type(X_train)

perceptron.fit(X_train_nparray, Y_train)

Y_pred_train = perceptron.predict(X_train_nparray)

print('accuracy for precition on train data: ', accuracy_score(Y_pred_train, Y_train))

plt.plot(perceptron.w)
plt.show()

# train the model with epoch
perceptron.fit_epoch(X_train_nparray, Y_train, 20)
Y_pred_train = perceptron.predict(X_train_nparray)

print('accuracy for precition on train data: ', accuracy_score(Y_pred_train, Y_train))

# train the model with epoch and checkpoint
perceptron.fit_epoch_chkpt(X_train_nparray, Y_train, 50)
Y_pred_train = perceptron.predict(X_train_nparray)

print('accuracy for precition on train data: ', accuracy_score(Y_pred_train, Y_train))

# run for 100 epochs
# train the model with epoch and checkpoint
perceptron.fit_epoch_chkpt(X_train_nparray, Y_train, 100)
Y_pred_train = perceptron.predict(X_train_nparray)

print('accuracy for precition on train data: ', accuracy_score(Y_pred_train, Y_train))

"""**Learning Rate**

---
"""

# learning rate - minimizes oscillation in graph
perceptron.fit_lr(X_train_nparray, Y_train, 1000)
Y_pred_train = perceptron.predict(X_train_nparray)

print('epoch:',1000,'learning rate:1 accuracy for precition on train data: ', accuracy_score(Y_pred_train, Y_train))

# learning rate - minimizes oscillation in graph
perceptron.fit_lr(X_train_nparray, Y_train, 1000, .0001)
Y_pred_train = perceptron.predict(X_train_nparray)

print('accuracy for precition on train data: ', accuracy_score(Y_pred_train, Y_train))

"""so far in perceptron the train accuracy has been focused on"""

plt.plot( perceptron.w )
plt.show()
# plt the final set of weights
# the weight value near to zero dont contribute much
# but large +ve and small -ve numbers play large role

"""animate the changing weights"""

wt_matrix = perceptron.fit_lr_animate(X_train_nparray, Y_train_nparray, 100)
plt.plot(wt_matrix[-1,:])
plt.show()
#this gives the values on x axis and y axis, to be used in animation

# Commented out IPython magic to ensure Python compatibility.
# http://louistiao.me/posts/notebooks/embedding-matplotlib-animations-in-jupyter-notebooks/
# %matplotlib inline
import numpy as np
import matplotlib.pyplot as plt

from matplotlib import animation, rc
from IPython.display import HTML
# First set up the figure, the axis, and the plot element we want to animate
fig, ax = plt.subplots()

ax.set_xlim( ( 0, wt_matrix.shape[1] )  ) # no of column in the wt_matrix or no of features
ax.set_ylim((-15000, 30000))

line, = ax.plot([], [], lw=2)


# initialization function: plot the background of each frame
def init():
    line.set_data([], [])
    return (line,)
# animation function. This is called sequentially
def animate(i):
    # x = np.linspace(0, 2, 1000)
    x = list( range(wt_matrix.shape[1]) ) # x axis points count equal to no of features
    # y = np.sin(2 * np.pi * (x - 0.01 * i))
    y = list(wt_matrix[i,:]) # all values in ith row of the weight matrix
    line.set_data(x, y)
    return (line,)
    
# call the animator. blit=True means only re-draw the parts that have changed.
# anim = animation.FuncAnimation(fig, animate, init_func=init, frames=100, interval=20, blit=True)

# time for magic
anim = animation.FuncAnimation(fig, animate, frames=100, interval=20, blit=True)
HTML(anim.to_html5_video())

# time for magic
anim = animation.FuncAnimation(fig, animate, frames=100, interval=100, blit=True)
HTML(anim.to_html5_video())

"""**Overfitting** : as the models become complex if the epoch no is very large, the value of weight may become very large or very small, causing issue called OVER-FITTING"""

